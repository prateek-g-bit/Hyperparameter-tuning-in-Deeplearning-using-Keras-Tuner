# Hyperparameter Tuning in Deep Learning ğŸ§ 

This repository contains a Jupyter Notebook that demonstrates how to perform **hyperparameter tuning** in deep learning using techniques like Grid Search, Random Search, and potentially Bayesian Optimization or Keras Tuner.

## ğŸ“ File Structure

- `hyperparameter_tuning_in_deep_learning.ipynb`: The main notebook where the deep learning model is built and hyperparameters are tuned.

## ğŸ” What This Project Covers

- Setting up a deep learning model using TensorFlow/Keras
- Understanding hyperparameters (e.g., learning rate, batch size, number of layers)
- Tuning hyperparameters using:
  - Manual tuning
  - Random Search
  - Keras Tuner 
- Evaluating performance (e.g., validation accuracy/loss)
- Understanding hyperparameters in Oprtimizers,Dropout Layers

## ğŸš€ How to Run

1. Clone the repository:
   ```bash
   git clone https://github.com/your-username/your-repo-name.git
   cd your-repo-name
## ğŸ“¦ Requirements
Ensure the following packages are installed:
numpy
pandas
matplotlib
tensorflow
keras
scikit-learn
keras_tuner
## ğŸ“Š Example Output
After tuning, the notebook will show the best hyperparameter combinations and their performance on the validation dataset.
